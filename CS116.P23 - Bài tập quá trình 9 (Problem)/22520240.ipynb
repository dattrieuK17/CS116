{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"07ecd6a9-a892-4160-b7e7-5c3ad8cfc632","cell_type":"code","source":"from sklearn.datasets import fetch_california_housing\nfrom sklearn.datasets import load_digits\nfrom sklearn.preprocessing import StandardScaler\nimport numpy as np\nimport time\nfrom typing import List, Tuple, Optional\nfrom multiprocessing import Pool, cpu_count\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, mean_squared_error, log_loss\nimport xgboost as xgb\nimport warnings","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T08:21:04.273074Z","iopub.execute_input":"2025-06-02T08:21:04.273483Z","iopub.status.idle":"2025-06-02T08:21:04.280994Z","shell.execute_reply.started":"2025-06-02T08:21:04.273453Z","shell.execute_reply":"2025-06-02T08:21:04.279641Z"}},"outputs":[],"execution_count":32},{"id":"4fe033d9-8e93-40df-bef8-67301e962b2e","cell_type":"markdown","source":"# Sá»­ dá»¥ng California Housing Dataset cho bÃ i toÃ¡n Regression","metadata":{}},{"id":"b39c089a-0d7f-4517-ae5a-3c568a1996d8","cell_type":"code","source":"data = fetch_california_housing()\n\nX_regression = data.data\ny_regression = data.target","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T08:21:04.282980Z","iopub.execute_input":"2025-06-02T08:21:04.283362Z","iopub.status.idle":"2025-06-02T08:21:04.315147Z","shell.execute_reply.started":"2025-06-02T08:21:04.283337Z","shell.execute_reply":"2025-06-02T08:21:04.314008Z"}},"outputs":[],"execution_count":33},{"id":"7970a3f7-f273-4a8a-aa41-bb5dfc116aeb","cell_type":"markdown","source":"# Sá»­ dá»¥ng Digit Dataset cho bÃ i toÃ¡n Binary Classification (chá»‰ phÃ¢n biá»‡t sá»‘ 3 hoáº·c khÃ´ng pháº£i sá»‘ 3)","metadata":{}},{"id":"168983cb-a0b7-4e8f-a8ce-2ff8afca6373","cell_type":"code","source":"data = load_digits()\n\nX_classification = data.data\ny_classification = (data.target == 3).astype(np.float32)  # nhá»‹ phÃ¢n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T08:21:04.317010Z","iopub.execute_input":"2025-06-02T08:21:04.317337Z","iopub.status.idle":"2025-06-02T08:21:04.338940Z","shell.execute_reply.started":"2025-06-02T08:21:04.317314Z","shell.execute_reply":"2025-06-02T08:21:04.337498Z"}},"outputs":[],"execution_count":34},{"id":"41bef9f0","cell_type":"markdown","source":"# Thuáº­t toÃ¡n 1: Histogram-based Algorithm.","metadata":{}},{"id":"bc0f3f3d","cell_type":"code","source":"def find_best_split_feature(bin_col: np.ndarray, grad: np.ndarray, hess: np.ndarray,\n                            feature_idx: int, lambda_l2: float, min_data_in_bin: int) -> Tuple[float, int, int]:\n    max_bin = bin_col.max() + 1\n    G_hist = np.bincount(bin_col, weights=grad, minlength=max_bin)\n    H_hist = np.bincount(bin_col, weights=hess, minlength=max_bin)\n\n    total_G = G_hist.sum()\n    total_H = H_hist.sum()\n    best_gain = -np.inf\n    best_threshold = -1\n\n    left_G = 0.0\n    left_H = 0.0\n    for t in range(1, max_bin):\n        left_G += G_hist[t - 1]\n        left_H += H_hist[t - 1]\n        right_G = total_G - left_G\n        right_H = total_H - left_H\n\n        if left_H < 1e-10 or right_H < 1e-10:\n            continue\n\n        gain = 0.5 * (left_G ** 2 / (left_H + lambda_l2) + right_G ** 2 / (right_H + lambda_l2))\n        gain -= 0.5 * (total_G ** 2 / (total_H + lambda_l2))\n\n        if gain > best_gain:\n            best_gain = gain\n            best_threshold = t\n\n    return best_gain, best_threshold, feature_idx\n\n\nclass HistogramBasedSplitter:\n    def __init__(self, max_bin=64, min_data_in_bin=5, lambda_l2=1.0, max_depth=2, parallel=True, task='regression'):\n        assert task in ('regression', 'classification')\n        self.max_bin = max_bin\n        self.min_data_in_bin = min_data_in_bin\n        self.lambda_l2 = lambda_l2\n        self.max_depth = max_depth\n        self.parallel = parallel\n        self.task = task\n        self.tree = {}\n        self.leaf_outputs = {}\n        self.bin_edges = None\n        self.feature_partitions = []\n\n    def _create_leaf(self, node: int, gradients: np.ndarray, hessians: np.ndarray):\n        G = np.sum(gradients)\n        H = np.sum(hessians)\n        output = -G / (H + self.lambda_l2) if H > 1e-10 else 0.0\n        self.leaf_outputs[node] = output\n\n    def _partition_features(self, n_features: int, n_partitions: int) -> List[List[int]]:\n        return [list(range(i, n_features, n_partitions)) for i in range(n_partitions)]\n\n    def fit(self, X: np.ndarray, y: np.ndarray, gradients=None, hessians=None):\n        n_samples, n_features = X.shape\n        if gradients is None or hessians is None:\n            if self.task == 'regression':\n                gradients = -2 * y\n                hessians = np.full_like(gradients, 2.0)\n            else:\n                gradients = 0.5 - y\n                hessians = np.full_like(gradients, 0.25)\n\n        self.bin_edges = np.percentile(X, np.linspace(0, 100, self.max_bin + 1), axis=0)\n        bin_indices = np.zeros_like(X, dtype=np.uint8)\n        for j in range(n_features):\n            bin_indices[:, j] = np.digitize(X[:, j], self.bin_edges[:, j]) - 1\n            bin_indices[:, j] = np.clip(bin_indices[:, j], 0, self.max_bin - 1)\n\n        n_workers = min(cpu_count(), n_features)\n        self.feature_partitions = self._partition_features(n_features, n_workers)\n\n        nodeSet = [0]\n        rowSet = {0: np.arange(n_samples)}\n\n        for depth in range(self.max_depth):\n            new_nodeSet = []\n            new_rowSet = {}\n\n            args_all = []\n            node_indices = []\n\n            for node in nodeSet:\n                used_rows = rowSet[node]\n                if len(used_rows) < 2 * self.min_data_in_bin:\n                    self._create_leaf(node, gradients[used_rows], hessians[used_rows])\n                    continue\n\n                for group in self.feature_partitions:\n                    for f in group:\n                        args_all.append((bin_indices[used_rows, f], gradients[used_rows], hessians[used_rows], f,\n                                         self.lambda_l2, self.min_data_in_bin))\n                        node_indices.append(node)\n\n            if not args_all:\n                break\n\n            if self.parallel:\n                with Pool(processes=n_workers) as pool:\n                    results = pool.starmap(find_best_split_feature, args_all)\n            else:\n                results = [find_best_split_feature(*arg) for arg in args_all]\n\n            best_splits: Dict[int, Tuple[float, int, int]] = {}\n            for idx, (gain, threshold, f) in enumerate(results):\n                node = node_indices[idx]\n                if node not in best_splits or gain > best_splits[node][0]:\n                    best_splits[node] = (gain, threshold, f)\n\n            for node in nodeSet:\n                used_rows = rowSet[node]\n                if node not in best_splits:\n                    self._create_leaf(node, gradients[used_rows], hessians[used_rows])\n                    continue\n\n                gain, threshold, best_feature = best_splits[node]\n                self.tree[node] = (best_feature, threshold)\n\n                left_mask = bin_indices[used_rows, best_feature] < threshold\n                right_mask = ~left_mask\n                left_rows = used_rows[left_mask]\n                right_rows = used_rows[right_mask]\n\n                left_id = node * 2 + 1\n                right_id = node * 2 + 2\n                new_nodeSet.extend([left_id, right_id])\n                new_rowSet[left_id] = left_rows\n                new_rowSet[right_id] = right_rows\n\n            nodeSet = new_nodeSet\n            rowSet = new_rowSet\n\n        for node in nodeSet:\n            used_rows = rowSet[node]\n            self._create_leaf(node, gradients[used_rows], hessians[used_rows])\n\n    def predict(self, X: np.ndarray) -> np.ndarray:\n        predictions = np.zeros(X.shape[0])\n        for i in range(X.shape[0]):\n            node = 0\n            while node in self.tree:\n                feature, threshold = self.tree[node]\n                bin_val = np.digitize(X[i, feature], self.bin_edges[:, feature]) - 1\n                bin_val = np.clip(bin_val, 0, self.max_bin - 1)\n                node = node * 2 + 1 if bin_val < threshold else node * 2 + 2\n            predictions[i] = self.leaf_outputs.get(node, 0.0)\n        return predictions\n\n    def print_tree(self, feature_names=None):\n        print(\"Tree structure:\")\n        for node, (feature, threshold) in self.tree.items():\n            name = feature_names[feature] if feature_names is not None else f\"Feature {feature}\"\n            print(f\" Node {node}: split on {name} at bin {threshold}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T08:21:04.340146Z","iopub.execute_input":"2025-06-02T08:21:04.340451Z","iopub.status.idle":"2025-06-02T08:21:04.367413Z","shell.execute_reply.started":"2025-06-02T08:21:04.340428Z","shell.execute_reply":"2025-06-02T08:21:04.366089Z"}},"outputs":[],"execution_count":35},{"id":"4a9d5589-ea80-4a1e-af2e-2648c71b1cc9","cell_type":"code","source":"# Huáº¥n luyá»‡n vá»›i cháº¿ Ä‘á»™ song song\nsplitter = HistogramBasedSplitter(max_depth=3, parallel=True, task='regression')\nsplitter.fit(X_regression, y_regression)\nsplitter.print_tree()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T08:21:04.368862Z","iopub.execute_input":"2025-06-02T08:21:04.369109Z","iopub.status.idle":"2025-06-02T08:21:04.646617Z","shell.execute_reply.started":"2025-06-02T08:21:04.369092Z","shell.execute_reply":"2025-06-02T08:21:04.645184Z"}},"outputs":[{"name":"stdout","text":"Tree structure:\n Node 0: split on Feature 0 at bin 50\n Node 1: split on Feature 0 at bin 25\n Node 2: split on Feature 0 at bin 60\n Node 3: split on Feature 2 at bin 14\n Node 4: split on Feature 5 at bin 14\n Node 5: split on Feature 5 at bin 22\n Node 6: split on Feature 0 at bin 62\n","output_type":"stream"}],"execution_count":36},{"id":"f1337fa2","cell_type":"code","source":"# Huáº¥n luyá»‡n vá»›i cháº¿ Ä‘á»™ song song\nsplitter = HistogramBasedSplitter(max_depth=3, parallel=True, task='classification')\nsplitter.fit(X_classification, y_classification)\nsplitter.print_tree()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T08:21:04.648753Z","iopub.execute_input":"2025-06-02T08:21:04.649078Z","iopub.status.idle":"2025-06-02T08:21:04.916285Z","shell.execute_reply.started":"2025-06-02T08:21:04.649051Z","shell.execute_reply":"2025-06-02T08:21:04.914808Z"}},"outputs":[{"name":"stdout","text":"Tree structure:\n Node 0: split on Feature 26 at bin 12\n Node 1: split on Feature 43 at bin 25\n Node 2: split on Feature 26 at bin 22\n Node 3: split on Feature 19 at bin 46\n Node 4: split on Feature 46 at bin 57\n Node 5: split on Feature 46 at bin 43\n Node 6: split on Feature 18 at bin 12\n","output_type":"stream"}],"execution_count":37},{"id":"80e68743-04fe-45bd-9e54-a1f5fad83b97","cell_type":"markdown","source":"# Thuáº­t toÃ¡n 2: Gradient-based One-Side Sampling (GOSS)","metadata":{}},{"id":"5bce0627","cell_type":"code","source":"class GOSS:\n    def __init__(self, top_rate: float = 0.2, other_rate: float = 0.1, task: str = 'regression'):\n        \"\"\"\n        top_rate: tá»· lá»‡ giá»¯ láº¡i cÃ¡c máº«u cÃ³ gradient lá»›n nháº¥t\n        other_rate: tá»· lá»‡ máº«u láº¥y ngáº«u nhiÃªn tá»« pháº§n gradient tháº¥p\n        task: 'regression' hoáº·c 'classification' (binary)\n        \"\"\"\n        assert 0 < top_rate < 1 and 0 < other_rate < 1, \"top_rate vÃ  other_rate pháº£i náº±m trong (0, 1)\"\n        assert task in ('regression', 'classification'), \"task pháº£i lÃ  'regression' hoáº·c 'classification'\"\n        self.top_rate = top_rate\n        self.other_rate = other_rate\n        self.task = task\n\n    def _compute_grad_hess(self, y: np.ndarray, preds: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n        if self.task == 'regression':\n            # MSE Loss: g = -2(y - f), h = 2\n            gradients = -2 * (y - preds)\n            hessians = np.full_like(gradients, 2.0)\n        elif self.task == 'classification':\n            # Binary log loss: g = p - y, h = p(1 - p)\n            preds = 1 / (1 + np.exp(-preds))  # sigmoid\n            gradients = preds - y\n            hessians = preds * (1 - preds)\n        return gradients, hessians\n\n    def sample(\n        self,\n        X: np.ndarray,\n        y: np.ndarray,\n        preds: np.ndarray\n    ) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n        \"\"\"\n        Tráº£ vá» táº­p con cá»§a dá»¯ liá»‡u sau khi Ã¡p dá»¥ng GOSS:\n        - X_sampled, y_sampled: Ä‘áº·c trÆ°ng vÃ  nhÃ£n\n        - gradients_sampled, hessians_sampled: gradient/hessian Ä‘Ã£ Ä‘Æ°á»£c scale\n        - weight: há»‡ sá»‘ scale cho má»—i máº«u\n        \"\"\"\n        gradients, hessians = self._compute_grad_hess(y, preds)\n        n_samples = len(gradients)\n        n_top = int(self.top_rate * n_samples)\n\n        # Step 1: Sáº¯p xáº¿p theo Ä‘á»™ lá»›n |gradient|\n        sorted_idx = np.argsort(-np.abs(gradients))\n        top_idx = sorted_idx[:n_top]\n        rest_idx = sorted_idx[n_top:]\n\n        # Step 2: Chá»n ngáº«u nhiÃªn tá»« pháº§n cÃ²n láº¡i\n        n_other = int(self.other_rate * n_samples)\n        n_other = min(n_other, len(rest_idx))\n        sampled_rest_idx = np.random.choice(rest_idx, n_other, replace=False)\n\n        # Step 3: Gá»™p láº¡i\n        selected_idx = np.concatenate([top_idx, sampled_rest_idx])\n        X_sampled = X[selected_idx]\n        y_sampled = y[selected_idx]\n        gradients_sampled = gradients[selected_idx]\n        hessians_sampled = hessians[selected_idx]\n\n        # Step 4: Trá»ng sá»‘ - scale pháº§n random\n        weight = np.ones_like(gradients_sampled)\n        is_rest = np.isin(selected_idx, sampled_rest_idx)\n        weight[is_rest] *= (1.0 / self.other_rate)\n\n        gradients_sampled *= weight\n        hessians_sampled *= weight\n\n        return X_sampled, y_sampled, gradients_sampled, hessians_sampled, weight\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T08:21:04.917920Z","iopub.execute_input":"2025-06-02T08:21:04.918242Z","iopub.status.idle":"2025-06-02T08:21:04.931225Z","shell.execute_reply.started":"2025-06-02T08:21:04.918213Z","shell.execute_reply":"2025-06-02T08:21:04.930006Z"}},"outputs":[],"execution_count":38},{"id":"243ff39e","cell_type":"code","source":"goss = GOSS(top_rate=0.2, other_rate=0.1, task='regression')\npreds = np.zeros_like(y_regression)\nX_s, y_s, g_s, h_s, w_s = goss.sample(X_regression, y_regression, preds)\n\nprint(f\"Tá»•ng sá»‘ máº«u ban Ä‘áº§u: {len(y_regression)}\")\nprint(f\"Sá»‘ máº«u sau GOSS: {len(y_s)}\")\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T08:21:04.933356Z","iopub.execute_input":"2025-06-02T08:21:04.933704Z","iopub.status.idle":"2025-06-02T08:21:04.961306Z","shell.execute_reply.started":"2025-06-02T08:21:04.933680Z","shell.execute_reply":"2025-06-02T08:21:04.960259Z"}},"outputs":[{"name":"stdout","text":"Tá»•ng sá»‘ máº«u ban Ä‘áº§u: 20640\nSá»‘ máº«u sau GOSS: 6192\n","output_type":"stream"}],"execution_count":39},{"id":"127e5729-0ebe-4b1c-8714-f26eb9b683e2","cell_type":"code","source":"goss = GOSS(top_rate=0.2, other_rate=0.1, task='classification')\npreds = np.zeros_like(y_classification)\nX_s, y_s, g_s, h_s, w_s = goss.sample(X_classification, y_classification, preds)\n\nprint(f\"Tá»•ng sá»‘ máº«u ban Ä‘áº§u: {len(y_classification)}\")\nprint(f\"Sá»‘ máº«u sau GOSS: {len(y_s)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T08:21:04.962702Z","iopub.execute_input":"2025-06-02T08:21:04.963713Z","iopub.status.idle":"2025-06-02T08:21:04.972246Z","shell.execute_reply.started":"2025-06-02T08:21:04.963683Z","shell.execute_reply":"2025-06-02T08:21:04.970910Z"}},"outputs":[{"name":"stdout","text":"Tá»•ng sá»‘ máº«u ban Ä‘áº§u: 1797\nSá»‘ máº«u sau GOSS: 538\n","output_type":"stream"}],"execution_count":40},{"id":"2e1e3a44-79c5-4a55-a011-22a11d7c50f5","cell_type":"markdown","source":"# Thuáº­t toÃ¡n 3: Exclusive Feature Bundling ","metadata":{}},{"id":"3743cd0b","cell_type":"code","source":"class ExclusiveFeatureBundler:\n    def __init__(self, max_conflict_count: int = 10):\n        \"\"\"\n        max_conflict_count: sá»‘ láº§n conflict tá»‘i Ä‘a giá»¯a má»™t feature vÃ  bundle Ä‘á»ƒ cho phÃ©p gá»™p.\n        \"\"\"\n        self.max_conflict_count = max_conflict_count\n        self.bundles: List[List[int]] = []\n\n    def _build_conflict_matrix(self, X: np.ndarray) -> np.ndarray:\n        \"\"\"\n        X: (n_samples, n_features)\n        Tráº£ vá» ma tráº­n xung Ä‘á»™t: conflict[i, j] = sá»‘ láº§n cáº£ X[:, i] vÃ  X[:, j] cÃ¹ng khÃ¡c 0\n        \"\"\"\n        n_features = X.shape[1]\n        conflict = np.zeros((n_features, n_features), dtype=int)\n        for i in range(n_features):\n            xi_nz = X[:, i] != 0\n            for j in range(i + 1, n_features):\n                xj_nz = X[:, j] != 0\n                conflict_count = np.logical_and(xi_nz, xj_nz).sum()\n                conflict[i, j] = conflict[j, i] = conflict_count\n        return conflict\n\n    def fit(self, X: np.ndarray) -> np.ndarray:\n        \"\"\"\n        Thá»±c hiá»‡n bundling theo thuáº­t toÃ¡n Greedy Bundling.\n        Tráº£ vá» X_bundle: ma tráº­n Ä‘áº·c trÆ°ng má»›i Ä‘Ã£ gá»™p\n        \"\"\"\n        n_features = X.shape[1]\n        conflict = self._build_conflict_matrix(X)\n\n        # TÃ­nh báº­c xung Ä‘á»™t cá»§a tá»«ng feature\n        degrees = conflict.sum(axis=1)\n        search_order = np.argsort(-degrees)  # Sáº¯p giáº£m dáº§n\n\n        used = np.zeros(n_features, dtype=bool)\n        bundles = []\n\n        for i in search_order:\n            if used[i]:\n                continue\n\n            need_new_bundle = True\n            for bundle in bundles:\n                # Tá»•ng conflict cá»§a i vá»›i bundle\n                total_conflict = sum(conflict[i, j] for j in bundle)\n                if total_conflict <= self.max_conflict_count:\n                    bundle.append(i)\n                    used[i] = True\n                    need_new_bundle = False\n                    break\n\n            if need_new_bundle:\n                bundles.append([i])\n                used[i] = True\n\n        self.bundles = bundles\n\n        # Táº¡o Ä‘áº·c trÆ°ng gá»™p\n        X_bundle = np.zeros((X.shape[0], len(bundles)))\n        for idx, bundle in enumerate(bundles):\n            X_bundle[:, idx] = X[:, bundle].sum(axis=1)\n\n        return X_bundle\n\n    def get_bundles(self) -> List[List[int]]:\n        return self.bundles\n\n    def transform(self, X: np.ndarray) -> np.ndarray:\n        \"\"\"THÃŠM Má»šI: Transform test data\"\"\"\n        X_bundle = np.zeros((X.shape[0], len(self.bundles)))\n        for idx, bundle in enumerate(self.bundles):\n            X_bundle[:, idx] = X[:, bundle].sum(axis=1)\n        return X_bundle\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T08:21:04.973523Z","iopub.execute_input":"2025-06-02T08:21:04.973915Z","iopub.status.idle":"2025-06-02T08:21:04.998189Z","shell.execute_reply.started":"2025-06-02T08:21:04.973884Z","shell.execute_reply":"2025-06-02T08:21:04.996915Z"}},"outputs":[],"execution_count":41},{"id":"66940c41","cell_type":"code","source":"bundler = ExclusiveFeatureBundler(max_conflict_count=3)\nX_bundle = bundler.fit(X_regression)\n\nprint(\"KÃ­ch thÆ°á»›c ban Ä‘áº§u:\", X_regression.shape)\nprint(\"KÃ­ch thÆ°á»›c má»›i:\", X_bundle.shape)\nprint(\"Bundle:\", bundler.get_bundles())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T08:21:04.999283Z","iopub.execute_input":"2025-06-02T08:21:04.999613Z","iopub.status.idle":"2025-06-02T08:21:05.027932Z","shell.execute_reply.started":"2025-06-02T08:21:04.999586Z","shell.execute_reply":"2025-06-02T08:21:05.026590Z"}},"outputs":[{"name":"stdout","text":"KÃ­ch thÆ°á»›c ban Ä‘áº§u: (20640, 8)\nKÃ­ch thÆ°á»›c má»›i: (20640, 8)\nBundle: [[0], [1], [2], [3], [4], [5], [6], [7]]\n","output_type":"stream"}],"execution_count":42},{"id":"4c1b951d-8a2c-4508-8320-a1e0ac127b40","cell_type":"code","source":"bundler = ExclusiveFeatureBundler(max_conflict_count=10)\nX_bundle = bundler.fit(X_classification)\n\nprint(\"KÃ­ch thÆ°á»›c ban Ä‘áº§u:\", X_classification.shape)\nprint(\"KÃ­ch thÆ°á»›c má»›i:\", X_bundle.shape)\nprint(\"Bundle:\", bundler.get_bundles())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T08:21:05.029233Z","iopub.execute_input":"2025-06-02T08:21:05.029620Z","iopub.status.idle":"2025-06-02T08:21:05.069943Z","shell.execute_reply.started":"2025-06-02T08:21:05.029589Z","shell.execute_reply":"2025-06-02T08:21:05.068843Z"}},"outputs":[{"name":"stdout","text":"KÃ­ch thÆ°á»›c ban Ä‘áº§u: (1797, 64)\nKÃ­ch thÆ°á»›c má»›i: (1797, 48)\nBundle: [[11, 40, 8, 48, 31, 16, 24, 56, 39, 32, 0], [4], [3], [12], [59], [60], [52], [51], [10], [18], [27], [36], [37], [26], [45], [28], [29], [53], [50, 23], [19], [13, 47], [35], [58], [2], [44], [34], [20], [61, 15], [21], [5], [43], [42], [46, 7], [38], [25], [17], [54], [33, 63], [9], [30], [22], [41], [14, 55], [62], [49], [6], [1], [57]]\n","output_type":"stream"}],"execution_count":43},{"id":"4d93ba33-6b99-47ef-8c02-f97992e53cc2","cell_type":"markdown","source":"# CÃ i Ä‘áº·t Simple LightGBM báº±ng 3 thuáº­t toÃ¡n trÃªn","metadata":{}},{"id":"112747f7-b25d-498c-a30a-15f3d08a95cf","cell_type":"code","source":"class SimpleLightGBM:\n    def __init__(self, \n                 n_estimators: int = 100,\n                 max_depth: int = 6,\n                 learning_rate: float = 0.1,\n                 task: str = 'regression',\n                 top_rate: float = 0.2,\n                 other_rate: float = 0.1,\n                 max_conflict_count: int = 10,\n                 max_bin: int = 64,\n                 min_data_in_bin: int = 5,\n                 lambda_l2: float = 1.0):\n        \n        self.n_estimators = n_estimators\n        self.max_depth = max_depth\n        self.learning_rate = learning_rate\n        self.task = task\n        \n        # Sá»­ dá»¥ng cÃ¡c thuáº­t toÃ¡n gá»‘c cá»§a báº¡n\n        self.bundler = ExclusiveFeatureBundler(max_conflict_count)\n        self.goss = GOSS(top_rate, other_rate, task)\n        \n        # Trees\n        self.trees = []\n        self.initial_prediction = 0.0\n        \n        # Parameters cho histogram\n        self.max_bin = max_bin\n        self.min_data_in_bin = min_data_in_bin\n        self.lambda_l2 = lambda_l2\n    \n    def fit(self, X: np.ndarray, y: np.ndarray):\n        print(f\"Original features: {X.shape[1]}\")\n        \n        # Step 1: Feature Bundling\n        X_bundled = self.bundler.fit(X)\n        print(f\"After bundling: {X_bundled.shape[1]} features\")\n        \n        # Initial prediction\n        if self.task == 'regression':\n            self.initial_prediction = np.mean(y)\n        else:\n            self.initial_prediction = np.log(np.mean(y) / (1 - np.mean(y) + 1e-8))\n        \n        predictions = np.full(len(y), self.initial_prediction)\n        \n        # Boosting\n        for i in range(self.n_estimators):\n            # Step 2: GOSS Sampling\n            X_sampled, y_sampled, gradients, hessians, weight = self.goss.sample(X_bundled, y, predictions)\n                \n            # Step 3: Histogram Tree\n            tree = HistogramBasedSplitter(\n                max_bin=self.max_bin,\n                min_data_in_bin=self.min_data_in_bin,\n                lambda_l2=self.lambda_l2,\n                max_depth=self.max_depth,\n                task=self.task,\n                parallel=False\n            )\n            \n            tree.fit(X_sampled, y_sampled, gradients, hessians)\n            self.trees.append(tree)\n            \n            # Update predictions\n            tree_preds = tree.predict(X_bundled)\n            predictions += self.learning_rate * tree_preds\n            \n            if (i + 1) % 20 == 0:\n                print(f\"Iteration {i + 1}/{self.n_estimators}\")\n                print(f\"Tá»•ng sá»‘ máº«u ban Ä‘áº§u: {len(y)}\")\n                print(f\"Sá»‘ máº«u sau GOSS: {len(y_sampled)}\")\n                tree.print_tree()\n \n    def predict(self, X: np.ndarray) -> np.ndarray:\n        # Transform features\n        X_bundled = self.bundler.transform(X)\n        \n        # Initial prediction\n        predictions = np.full(X.shape[0], self.initial_prediction)\n        \n        # Add tree predictions\n        for tree in self.trees:\n            tree_preds = tree.predict(X_bundled)\n            predictions += self.learning_rate * tree_preds\n        \n        if self.task == 'classification':\n            predictions = 1 / (1 + np.exp(-predictions))\n        \n        return predictions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T08:21:05.071373Z","iopub.execute_input":"2025-06-02T08:21:05.071716Z","iopub.status.idle":"2025-06-02T08:21:05.085480Z","shell.execute_reply.started":"2025-06-02T08:21:05.071686Z","shell.execute_reply":"2025-06-02T08:21:05.084430Z"}},"outputs":[],"execution_count":44},{"id":"4bbbc6a3-9977-419a-8213-a851047096ef","cell_type":"markdown","source":"# ÄÃ¡nh giÃ¡ vá»›i XGBoost","metadata":{}},{"id":"30417596-98e2-4af4-b372-8056c46d45ba","cell_type":"code","source":"def compare_algorithms():\n    print(\"=\" * 60)\n    print(\"COMPARISON: SimpleLightGBM vs XGBoost\")\n    print(\"=\" * 60)\n    \n    # Test Regression\n    print(\"\\nğŸ”¹ REGRESSION TASK\")\n    print(\"-\" * 40)\n    \n    X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X_regression, y_regression, test_size=0.2, random_state=42)\n    \n    # SimpleLightGBM\n    print(\"\\nğŸ“Š Training SimpleLightGBM...\")\n    start_time = time.time()\n    lgb_model = SimpleLightGBM(n_estimators=50, task='regression', learning_rate=0.1, max_depth=4)\n    lgb_model.fit(X_train_reg, y_train_reg)\n    lgb_train_time = time.time() - start_time\n    \n    start_time = time.time()\n    lgb_preds = lgb_model.predict(X_test_reg)\n    lgb_pred_time = time.time() - start_time\n    lgb_mse = mean_squared_error(y_test_reg, lgb_preds)\n    \n    # XGBoost\n    print(\"\\nğŸ“Š Training XGBoost...\")\n    start_time = time.time()\n    xgb_model = xgb.XGBRegressor(n_estimators=50, max_depth=4, learning_rate=0.1, random_state=42)\n    xgb_model.fit(X_train_reg, y_train_reg)\n    xgb_train_time = time.time() - start_time\n    \n    start_time = time.time()\n    xgb_preds = xgb_model.predict(X_test_reg)\n    xgb_pred_time = time.time() - start_time\n    xgb_mse = mean_squared_error(y_test_reg, xgb_preds)\n    \n    print(f\"\\nğŸ“ˆ REGRESSION RESULTS:\")\n    print(f\"SimpleLightGBM - Train: {lgb_train_time:.2f}s, Predict: {lgb_pred_time:.4f}s, MSE: {lgb_mse:.4f}\")\n    print(f\"XGBoost       - Train: {xgb_train_time:.2f}s, Predict: {xgb_pred_time:.4f}s, MSE: {xgb_mse:.4f}\")\n    \n    # Test Classification\n    print(\"\\nğŸ”¹ CLASSIFICATION TASK\")\n    print(\"-\" * 40)\n    \n    X_train_cls, X_test_cls, y_train_cls, y_test_cls = train_test_split(X_classification, y_classification, test_size=0.2, random_state=42)\n    \n    # SimpleLightGBM\n    print(\"\\nğŸ“Š Training SimpleLightGBM...\")\n    start_time = time.time()\n    lgb_cls_model = SimpleLightGBM(n_estimators=50, task='classification', learning_rate=0.1, max_depth=4)\n    lgb_cls_model.fit(X_train_cls, y_train_cls)\n    lgb_cls_train_time = time.time() - start_time\n    \n    start_time = time.time()\n    lgb_cls_preds = lgb_cls_model.predict(X_test_cls)\n    lgb_cls_pred_time = time.time() - start_time\n    lgb_cls_acc = accuracy_score(y_test_cls, lgb_cls_preds > 0.5)\n    \n    # XGBoost\n    print(\"\\nğŸ“Š Training XGBoost...\")\n    start_time = time.time()\n    xgb_cls_model = xgb.XGBClassifier(n_estimators=50, max_depth=4, learning_rate=0.1, random_state=42)\n    xgb_cls_model.fit(X_train_cls, y_train_cls)\n    xgb_cls_train_time = time.time() - start_time\n    \n    start_time = time.time()\n    xgb_cls_preds = xgb_cls_model.predict_proba(X_test_cls)[:, 1]\n    xgb_cls_pred_time = time.time() - start_time\n    xgb_cls_acc = accuracy_score(y_test_cls, xgb_cls_preds > 0.5)\n    \n    print(f\"\\nğŸ“ˆ CLASSIFICATION RESULTS:\")\n    print(f\"SimpleLightGBM - Train: {lgb_cls_train_time:.2f}s, Predict: {lgb_cls_pred_time:.4f}s, Accuracy: {lgb_cls_acc:.4f}\")\n    print(f\"XGBoost       - Train: {xgb_cls_train_time:.2f}s, Predict: {xgb_cls_pred_time:.4f}s, Accuracy: {xgb_cls_acc:.4f}\")\n    \n    \n    print(\"\\n\" + \"=\" * 60)\n    print(\"ğŸ¯ SUMMARY\")\n    print(\"=\" * 60)\n    print(\"SimpleLightGBM sá»­ dá»¥ng Ä‘Ãºng 3 thuáº­t toÃ¡n gá»‘c:\")\n    print(\"âœ… Histogram-Based Splitter\")\n    print(\"âœ… GOSS\")  \n    print(\"âœ… Exclusive Feature Bundler\")\n    print(f\"\\nKáº¿t quáº£:\")\n    print(f\"â€¢ MSE tÆ°Æ¡ng Ä‘Æ°Æ¡ng XGBoost\")\n    print(f\"â€¢ Accuracy tÆ°Æ¡ng Ä‘Æ°Æ¡ng XGBoost\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T08:21:05.088196Z","iopub.execute_input":"2025-06-02T08:21:05.088440Z","iopub.status.idle":"2025-06-02T08:21:05.108778Z","shell.execute_reply.started":"2025-06-02T08:21:05.088422Z","shell.execute_reply":"2025-06-02T08:21:05.107658Z"}},"outputs":[],"execution_count":45},{"id":"91f075b4-6ba7-476a-a6d9-7957f53dfbb4","cell_type":"code","source":"compare_algorithms()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-02T08:21:05.110232Z","iopub.execute_input":"2025-06-02T08:21:05.110739Z","iopub.status.idle":"2025-06-02T08:21:58.810855Z","shell.execute_reply.started":"2025-06-02T08:21:05.110715Z","shell.execute_reply":"2025-06-02T08:21:58.808777Z"}},"outputs":[{"name":"stdout","text":"============================================================\nCOMPARISON: SimpleLightGBM vs XGBoost\n============================================================\n\nğŸ”¹ REGRESSION TASK\n----------------------------------------\n\nğŸ“Š Training SimpleLightGBM...\nOriginal features: 8\nAfter bundling: 8 features\nIteration 20/50\nTá»•ng sá»‘ máº«u ban Ä‘áº§u: 16512\nSá»‘ máº«u sau GOSS: 4953\nTree structure:\n Node 0: split on Feature 0 at bin 21\n Node 1: split on Feature 6 at bin 35\n Node 2: split on Feature 7 at bin 7\n Node 3: split on Feature 7 at bin 43\n Node 4: split on Feature 7 at bin 18\n Node 5: split on Feature 6 at bin 58\n Node 6: split on Feature 6 at bin 48\n Node 7: split on Feature 5 at bin 12\n Node 8: split on Feature 7 at bin 63\n Node 9: split on Feature 6 at bin 57\n Node 10: split on Feature 7 at bin 25\n Node 11: split on Feature 6 at bin 52\n Node 12: split on Feature 6 at bin 61\n Node 13: split on Feature 7 at bin 49\n Node 14: split on Feature 4 at bin 33\nIteration 40/50\nTá»•ng sá»‘ máº«u ban Ä‘áº§u: 16512\nSá»‘ máº«u sau GOSS: 4953\nTree structure:\n Node 0: split on Feature 7 at bin 6\n Node 1: split on Feature 6 at bin 62\n Node 2: split on Feature 6 at bin 48\n Node 3: split on Feature 3 at bin 24\n Node 4: split on Feature 3 at bin 41\n Node 5: split on Feature 7 at bin 13\n Node 6: split on Feature 1 at bin 26\n Node 7: split on Feature 1 at bin 55\n Node 8: split on Feature 7 at bin 5\n Node 9: split on Feature 0 at bin 22\n Node 10: split on Feature 4 at bin 2\n Node 11: split on Feature 6 at bin 43\n Node 12: split on Feature 6 at bin 36\n Node 13: split on Feature 5 at bin 10\n Node 14: split on Feature 2 at bin 59\n\nğŸ“Š Training XGBoost...\n\nğŸ“ˆ REGRESSION RESULTS:\nSimpleLightGBM - Train: 36.99s, Predict: 9.0461s, MSE: 0.2983\nXGBoost       - Train: 0.08s, Predict: 0.0029s, MSE: 0.3009\n\nğŸ”¹ CLASSIFICATION TASK\n----------------------------------------\n\nğŸ“Š Training SimpleLightGBM...\nOriginal features: 64\nAfter bundling: 48 features\nIteration 20/50\nTá»•ng sá»‘ máº«u ban Ä‘áº§u: 1437\nSá»‘ máº«u sau GOSS: 430\nTree structure:\n Node 0: split on Feature 26 at bin 36\n Node 1: split on Feature 1 at bin 14\n Node 2: split on Feature 37 at bin 50\n Node 3: split on Feature 14 at bin 32\n Node 4: split on Feature 13 at bin 51\n Node 5: split on Feature 13 at bin 27\n Node 6: split on Feature 18 at bin 34\n Node 7: split on Feature 33 at bin 56\n Node 8: split on Feature 22 at bin 31\n Node 9: split on Feature 17 at bin 52\n Node 10: split on Feature 9 at bin 26\n Node 11: split on Feature 16 at bin 29\n Node 12: split on Feature 45 at bin 63\n Node 13: split on Feature 0 at bin 26\nIteration 40/50\nTá»•ng sá»‘ máº«u ban Ä‘áº§u: 1437\nSá»‘ máº«u sau GOSS: 430\nTree structure:\n Node 0: split on Feature 17 at bin 48\n Node 1: split on Feature 31 at bin 45\n Node 2: split on Feature 19 at bin 52\n Node 3: split on Feature 30 at bin 53\n Node 4: split on Feature 45 at bin 57\n Node 5: split on Feature 46 at bin 56\n Node 6: split on Feature 10 at bin 11\n Node 7: split on Feature 24 at bin 16\n Node 8: split on Feature 7 at bin 29\n Node 9: split on Feature 12 at bin 46\n Node 11: split on Feature 35 at bin 62\n Node 14: split on Feature 45 at bin 55\n\nğŸ“Š Training XGBoost...\n\nğŸ“ˆ CLASSIFICATION RESULTS:\nSimpleLightGBM - Train: 6.71s, Predict: 0.7856s, Accuracy: 0.9917\nXGBoost       - Train: 0.05s, Predict: 0.0021s, Accuracy: 0.9889\n\n============================================================\nğŸ¯ SUMMARY\n============================================================\nSimpleLightGBM sá»­ dá»¥ng Ä‘Ãºng 3 thuáº­t toÃ¡n gá»‘c:\nâœ… Histogram-Based Splitter\nâœ… GOSS\nâœ… Exclusive Feature Bundler\n\nKáº¿t quáº£:\nâ€¢ MSE tÆ°Æ¡ng Ä‘Æ°Æ¡ng XGBoost\nâ€¢ Accuracy tÆ°Æ¡ng Ä‘Æ°Æ¡ng XGBoost\n","output_type":"stream"}],"execution_count":46}]}